{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efd795a1"
   },
   "source": [
    "# Assessment Problem\n",
    "Write a Python script that processes sensor data from a zip file (`sensor_data_2025.zip`). The script should:\n",
    "1. Extract data from nested city zip files (e.g., `munich_data.zip`, `tokyo_data.zip`), reading ``readings.csv`` and `station_info.json` from each.\n",
    "2. Add a `city` column to each DataFrame with the city's name.\n",
    "3. Combine the data into two main DataFrames: one for readings and one for station info.\n",
    "4. Merge these two DataFrames on `station_id`.\n",
    "5. Clean the merged data by keeping only `active` stations and removing rows with missing `temperature_celsius` or `pm2_5`.\n",
    "6. Calculate the average temperature and PM2.5 for each city.\n",
    "7. Save the city summary as a CSV file named `city_summary_report.csv` with columns `city`, `average_temperature`, and `average_pm2_5`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3a3730b0"
   },
   "source": [
    "## TASK 1: Load and combine data\n",
    "\n",
    "Go through each city's zip file, read the `readings.csv` and `station_info.json` files, add a 'city' column to each DataFrame, and combine them into `all_readings_df` and `all_station_info_df`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e84b1b8c"
   },
   "source": [
    "**Reasoning**:\n",
    "TASK 1 requires extracting data from nested zip files, processing them, and combining them into two dataframes. This involves using the zipfile library and pandas for data manipulation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "bb305077"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['station_info.json', 'readings.csv']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'city name'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[147]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m station_info=pd.read_json(\u001b[33m\"\u001b[39m\u001b[33mstation_info.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m station_info\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m station_info[\u001b[33m'\u001b[39m\u001b[33mcity\u001b[39m\u001b[33m'\u001b[39m]=\u001b[43mstation_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcity name\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m reading[\u001b[33m'\u001b[39m\u001b[33mcity\u001b[39m\u001b[33m'\u001b[39m]=reading.groupby(\u001b[33m'\u001b[39m\u001b[33mcity name\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:9183\u001b[39m, in \u001b[36mDataFrame.groupby\u001b[39m\u001b[34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m   9180\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   9181\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou have to supply one of \u001b[39m\u001b[33m'\u001b[39m\u001b[33mby\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlevel\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m9183\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   9184\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   9185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9186\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9189\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9193\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1329\u001b[39m, in \u001b[36mGroupBy.__init__\u001b[39m\u001b[34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28mself\u001b[39m.dropna = dropna\n\u001b[32m   1328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m     grouper, exclusions, obj = \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1339\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib.no_default:\n\u001b[32m   1340\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping._passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper.groupings):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:1043\u001b[39m, in \u001b[36mget_grouper\u001b[39m\u001b[34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[39m\n\u001b[32m   1041\u001b[39m         in_axis, level, gpr = \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1042\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[32m   1044\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr.key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1045\u001b[39m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[32m   1046\u001b[39m     exclusions.add(gpr.key)\n",
      "\u001b[31mKeyError\u001b[39m: 'city name'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#unzip sensor_data\n",
    "import zipfile\n",
    "def unzip(filename):\n",
    "    with zipfile.ZipFile(filename,'r') as zf:\n",
    "        inner_list=zf.namelist()\n",
    "        zf.extractall()\n",
    "        return inner_list\n",
    "inner_list=unzip(\"sensor_data_2025.zip\")\n",
    "for file in inner_list:\n",
    "    unzip(file)\n",
    "\n",
    "#extracting all files\n",
    "with zipfile.ZipFile(\"munich_data.zip\",'r') as zf:\n",
    "        zf.extractall()\n",
    "\n",
    "#extracting all files in tokyo_data\n",
    "with zipfile.ZipFile(\"tokyo_data.zip\",'r') as z:\n",
    "        z.extractall()\n",
    "        print(z.namelist())\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "reading=pd.read_csv(\"readings.csv\")\n",
    "station_info=pd.read_json(\"station_info.json\")\n",
    "station_info\n",
    "station_info['city']=station_info.groupby('city name')\n",
    "reading['city']=reading.groupby('city name')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7aacbc0"
   },
   "source": [
    "## TASK 2: Merge dataframes\n",
    "\n",
    "Merge the `all_readings_df` and `all_station_info_df` into a single DataFrame using the `station_id` column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dfaad49"
   },
   "source": [
    "**Reasoning**:\n",
    "Merge the two dataframes `all_readings_df` and `all_station_info_df` on the `station_id` column and display the head of the resulting dataframe to verify the merge.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "cf658701"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>location</th>\n",
       "      <th>status</th>\n",
       "      <th>deployment_year</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>temperature_celsius</th>\n",
       "      <th>humidity_percent</th>\n",
       "      <th>pm2_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1006</td>\n",
       "      <td>{'latitude': 35.668296, 'longitude': 139.765741}</td>\n",
       "      <td>active</td>\n",
       "      <td>2018</td>\n",
       "      <td>2025-03-05 23:00:00</td>\n",
       "      <td>11.14</td>\n",
       "      <td>92.50</td>\n",
       "      <td>43.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1006</td>\n",
       "      <td>{'latitude': 35.668296, 'longitude': 139.765741}</td>\n",
       "      <td>active</td>\n",
       "      <td>2018</td>\n",
       "      <td>2025-03-31 01:00:00</td>\n",
       "      <td>9.97</td>\n",
       "      <td>94.61</td>\n",
       "      <td>34.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1006</td>\n",
       "      <td>{'latitude': 35.668296, 'longitude': 139.765741}</td>\n",
       "      <td>active</td>\n",
       "      <td>2018</td>\n",
       "      <td>2025-01-14 09:00:00</td>\n",
       "      <td>18.95</td>\n",
       "      <td>54.45</td>\n",
       "      <td>36.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1006</td>\n",
       "      <td>{'latitude': 35.668296, 'longitude': 139.765741}</td>\n",
       "      <td>active</td>\n",
       "      <td>2018</td>\n",
       "      <td>2025-02-01 02:00:00</td>\n",
       "      <td>16.94</td>\n",
       "      <td>61.41</td>\n",
       "      <td>13.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1006</td>\n",
       "      <td>{'latitude': 35.668296, 'longitude': 139.765741}</td>\n",
       "      <td>active</td>\n",
       "      <td>2018</td>\n",
       "      <td>2025-01-27 14:00:00</td>\n",
       "      <td>14.38</td>\n",
       "      <td>54.03</td>\n",
       "      <td>10.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1010</td>\n",
       "      <td>{'latitude': 35.61944, 'longitude': 139.76487}</td>\n",
       "      <td>active</td>\n",
       "      <td>2020</td>\n",
       "      <td>2025-01-19 17:00:00</td>\n",
       "      <td>10.60</td>\n",
       "      <td>34.62</td>\n",
       "      <td>45.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>1010</td>\n",
       "      <td>{'latitude': 35.61944, 'longitude': 139.76487}</td>\n",
       "      <td>active</td>\n",
       "      <td>2020</td>\n",
       "      <td>2025-01-17 22:00:00</td>\n",
       "      <td>8.13</td>\n",
       "      <td>43.52</td>\n",
       "      <td>28.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>1010</td>\n",
       "      <td>{'latitude': 35.61944, 'longitude': 139.76487}</td>\n",
       "      <td>active</td>\n",
       "      <td>2020</td>\n",
       "      <td>2025-03-13 07:00:00</td>\n",
       "      <td>19.56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>1010</td>\n",
       "      <td>{'latitude': 35.61944, 'longitude': 139.76487}</td>\n",
       "      <td>active</td>\n",
       "      <td>2020</td>\n",
       "      <td>2025-03-28 00:00:00</td>\n",
       "      <td>5.79</td>\n",
       "      <td>61.62</td>\n",
       "      <td>18.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>1010</td>\n",
       "      <td>{'latitude': 35.61944, 'longitude': 139.76487}</td>\n",
       "      <td>active</td>\n",
       "      <td>2020</td>\n",
       "      <td>2025-02-21 07:00:00</td>\n",
       "      <td>9.06</td>\n",
       "      <td>52.05</td>\n",
       "      <td>59.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     station_id                                          location  status  \\\n",
       "0          1006  {'latitude': 35.668296, 'longitude': 139.765741}  active   \n",
       "1          1006  {'latitude': 35.668296, 'longitude': 139.765741}  active   \n",
       "2          1006  {'latitude': 35.668296, 'longitude': 139.765741}  active   \n",
       "3          1006  {'latitude': 35.668296, 'longitude': 139.765741}  active   \n",
       "4          1006  {'latitude': 35.668296, 'longitude': 139.765741}  active   \n",
       "..          ...                                               ...     ...   \n",
       "495        1010    {'latitude': 35.61944, 'longitude': 139.76487}  active   \n",
       "496        1010    {'latitude': 35.61944, 'longitude': 139.76487}  active   \n",
       "497        1010    {'latitude': 35.61944, 'longitude': 139.76487}  active   \n",
       "498        1010    {'latitude': 35.61944, 'longitude': 139.76487}  active   \n",
       "499        1010    {'latitude': 35.61944, 'longitude': 139.76487}  active   \n",
       "\n",
       "     deployment_year            timestamp  temperature_celsius  \\\n",
       "0               2018  2025-03-05 23:00:00                11.14   \n",
       "1               2018  2025-03-31 01:00:00                 9.97   \n",
       "2               2018  2025-01-14 09:00:00                18.95   \n",
       "3               2018  2025-02-01 02:00:00                16.94   \n",
       "4               2018  2025-01-27 14:00:00                14.38   \n",
       "..               ...                  ...                  ...   \n",
       "495             2020  2025-01-19 17:00:00                10.60   \n",
       "496             2020  2025-01-17 22:00:00                 8.13   \n",
       "497             2020  2025-03-13 07:00:00                19.56   \n",
       "498             2020  2025-03-28 00:00:00                 5.79   \n",
       "499             2020  2025-02-21 07:00:00                 9.06   \n",
       "\n",
       "     humidity_percent  pm2_5  \n",
       "0               92.50  43.29  \n",
       "1               94.61  34.22  \n",
       "2               54.45  36.74  \n",
       "3               61.41  13.78  \n",
       "4               54.03  10.38  \n",
       "..                ...    ...  \n",
       "495             34.62  45.38  \n",
       "496             43.52  28.63  \n",
       "497               NaN  56.10  \n",
       "498             61.62  18.84  \n",
       "499             52.05  59.33  \n",
       "\n",
       "[500 rows x 8 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge=pd.merge(station_info,reading,on='station_id')\n",
    "merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f453b143"
   },
   "source": [
    "## TASK 3: Clean data\n",
    "\n",
    "Filter the merged DataFrame to keep only 'active' stations and remove rows with missing values in 'temperature_celsius' or 'pm2_5'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88ecd5a3"
   },
   "source": [
    "**Reasoning**:\n",
    "Filter the merged dataframe to keep only active stations and remove rows with missing values in the specified columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "fa0aa6f0"
   },
   "outputs": [],
   "source": [
    "merge=merge.dropna(subset=['temperature_celsius','pm2_5'],how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b6a3be6"
   },
   "source": [
    "## TASK 4: Analyze data\n",
    "\n",
    "Calculate the average 'temperature_celsius' and 'pm2_5' for each city.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "485603b4"
   },
   "source": [
    "**Reasoning**:\n",
    "Calculate the mean temperature and PM2.5 for each city by grouping the cleaned_df by city and applying the mean aggregation. Then display the head of the resulting dataframe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "ac870825"
   },
   "outputs": [],
   "source": [
    "mean_temperature=merge['temperature_celsius'].agg(['mean'])\n",
    "mean_city=merge['pm2_5'].agg(['mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40979f5f"
   },
   "source": [
    "## TASK 5: Save report\n",
    "\n",
    "Save the city summary as a CSV file named `city_summary_report.csv` with the specified columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1842ff9e"
   },
   "source": [
    "**Reasoning**:\n",
    "Reset the index, rename the city column, and save the city summary DataFrame to a CSV file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "2d720f4f"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified: 'merged.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[164]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m mean=mean_city.to_csv(\u001b[33m\"\u001b[39m\u001b[33mmean_city\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m zipfile.ZipFile(\u001b[33m\"\u001b[39m\u001b[33mcity_summary_report.zip\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;28;01mas\u001b[39;00m zf:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[43mzf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmerged.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     zf.write(\u001b[33m\"\u001b[39m\u001b[33mmeaned.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m     zf.write(\u001b[33m\"\u001b[39m\u001b[33mmean.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\Lib\\zipfile\\__init__.py:1889\u001b[39m, in \u001b[36mZipFile.write\u001b[39m\u001b[34m(self, filename, arcname, compress_type, compresslevel)\u001b[39m\n\u001b[32m   1884\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._writing:\n\u001b[32m   1885\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1886\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt write to ZIP archive while an open writing handle exists\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1887\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1889\u001b[39m zinfo = \u001b[43mZipInfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marcname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mstrict_timestamps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_strict_timestamps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1892\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m zinfo.is_dir():\n\u001b[32m   1893\u001b[39m     zinfo.compress_size = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\Lib\\zipfile\\__init__.py:580\u001b[39m, in \u001b[36mZipInfo.from_file\u001b[39m\u001b[34m(cls, filename, arcname, strict_timestamps)\u001b[39m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename, os.PathLike):\n\u001b[32m    579\u001b[39m     filename = os.fspath(filename)\n\u001b[32m--> \u001b[39m\u001b[32m580\u001b[39m st = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    581\u001b[39m isdir = stat.S_ISDIR(st.st_mode)\n\u001b[32m    582\u001b[39m mtime = time.localtime(st.st_mtime)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 2] The system cannot find the file specified: 'merged.csv'"
     ]
    }
   ],
   "source": [
    "merged=merge.to_csv(\"merge\")\n",
    "meaned=mean_temperature.to_csv(\"mean_temperature\")\n",
    "mean=mean_city.to_csv(\"mean_city\")\n",
    "with zipfile.ZipFile(\"city_summary_report.zip\",'w')as zf:\n",
    "    zf.write(\"merged.csv\")\n",
    "    zf.write(\"meaned.csv\")\n",
    "    zf.write(\"mean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target not found\n",
      "[2, 3, 12, 34]\n"
     ]
    }
   ],
   "source": [
    "#linear\n",
    "def linear(arr,target):\n",
    "    for i in range(len(arr)):\n",
    "        if arr[i]==target:\n",
    "            return i\n",
    "    return -1\n",
    "nums=[2,1,3,4]\n",
    "target=30\n",
    "result=linear(nums,target)\n",
    "\n",
    "if result !=-1:\n",
    "    print(f\"Target{target} found at {\"result\"}\") \n",
    "else:\n",
    "    print(\"Target not found\")\n",
    "    \n",
    "#bubble sort\n",
    "def bubble(arr):\n",
    "    for i in range(len(arr)):\n",
    "        for j in range(len(arr)-1):\n",
    "            if arr[j]>arr[j+1]:\n",
    "                arr[j],arr[j+1]=arr[j+1],arr[j]\n",
    "nums=[2,34,12,3]\n",
    "bubble(nums)\n",
    "print(nums)\n",
    "            \n",
    "\n",
    "\n",
    "                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
