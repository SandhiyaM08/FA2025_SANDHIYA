{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efd795a1"
   },
   "source": [
    "# Assessment Problem\n",
    "Write a Python script that processes sensor data from a zip file (`sensor_data_2025.zip`). The script should:\n",
    "1. Extract data from nested city zip files (e.g., `munich_data.zip`, `tokyo_data.zip`), reading ``readings.csv`` and `station_info.json` from each.\n",
    "2. Add a `city` column to each DataFrame with the city's name.\n",
    "3. Combine the data into two main DataFrames: one for readings and one for station info.\n",
    "4. Merge these two DataFrames on `station_id`.\n",
    "5. Clean the merged data by keeping only `active` stations and removing rows with missing `temperature_celsius` or `pm2_5`.\n",
    "6. Calculate the average temperature and PM2.5 for each city.\n",
    "7. Save the city summary as a CSV file named `city_summary_report.csv` with columns `city`, `average_temperature`, and `average_pm2_5`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3a3730b0"
   },
   "source": [
    "## TASK 1: Load and combine data\n",
    "\n",
    "Go through each city's zip file, read the `readings.csv` and `station_info.json` files, add a 'city' column to each DataFrame, and combine them into `all_readings_df` and `all_station_info_df`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e84b1b8c"
   },
   "source": [
    "**Reasoning**:\n",
    "TASK 1 requires extracting data from nested zip files, processing them, and combining them into two dataframes. This involves using the zipfile library and pandas for data manipulation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "bb305077"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['station_info.json', 'readings.csv']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'city'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'city'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[103]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m station_info=pd.read_json(\u001b[33m\"\u001b[39m\u001b[33mstation_info.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m station_info\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m station_info[\u001b[33m'\u001b[39m\u001b[33mcity\u001b[39m\u001b[33m'\u001b[39m]=\u001b[43mstation_info\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcity\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.fillna(\u001b[33m'\u001b[39m\u001b[33mTrue\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'city'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#unzip sensor_data\n",
    "import zipfile\n",
    "def unzip(filename):\n",
    "    with zipfile.ZipFile(filename,'r') as zf:\n",
    "        inner_list=zf.namelist()\n",
    "        zf.extractall()\n",
    "        return inner_list\n",
    "inner_list=unzip(\"sensor_data_2025.zip\")\n",
    "for file in inner_list:\n",
    "    unzip(file)\n",
    "\n",
    "#extracting all files\n",
    "with zipfile.ZipFile(\"munich_data.zip\",'r') as zf:\n",
    "        zf.extractall()\n",
    "\n",
    "#extracting all files in tokyo_data\n",
    "with zipfile.ZipFile(\"tokyo_data.zip\",'r') as z:\n",
    "        z.extractall()\n",
    "        print(z.namelist())\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "reading=pd.read_csv(\"readings.csv\")\n",
    "station_info=pd.read_json(\"station_info.json\")\n",
    "station_info\n",
    "station_info['city']=station_info['city']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7aacbc0"
   },
   "source": [
    "## TASK 2: Merge dataframes\n",
    "\n",
    "Merge the `all_readings_df` and `all_station_info_df` into a single DataFrame using the `station_id` column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dfaad49"
   },
   "source": [
    "**Reasoning**:\n",
    "Merge the two dataframes `all_readings_df` and `all_station_info_df` on the `station_id` column and display the head of the resulting dataframe to verify the merge.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "cf658701"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>location</th>\n",
       "      <th>status</th>\n",
       "      <th>deployment_year</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>temperature_celsius</th>\n",
       "      <th>humidity_percent</th>\n",
       "      <th>pm2_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1006</td>\n",
       "      <td>{'latitude': 35.668296, 'longitude': 139.765741}</td>\n",
       "      <td>active</td>\n",
       "      <td>2018</td>\n",
       "      <td>2025-03-05 23:00:00</td>\n",
       "      <td>11.14</td>\n",
       "      <td>92.50</td>\n",
       "      <td>43.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1006</td>\n",
       "      <td>{'latitude': 35.668296, 'longitude': 139.765741}</td>\n",
       "      <td>active</td>\n",
       "      <td>2018</td>\n",
       "      <td>2025-03-31 01:00:00</td>\n",
       "      <td>9.97</td>\n",
       "      <td>94.61</td>\n",
       "      <td>34.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1006</td>\n",
       "      <td>{'latitude': 35.668296, 'longitude': 139.765741}</td>\n",
       "      <td>active</td>\n",
       "      <td>2018</td>\n",
       "      <td>2025-01-14 09:00:00</td>\n",
       "      <td>18.95</td>\n",
       "      <td>54.45</td>\n",
       "      <td>36.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1006</td>\n",
       "      <td>{'latitude': 35.668296, 'longitude': 139.765741}</td>\n",
       "      <td>active</td>\n",
       "      <td>2018</td>\n",
       "      <td>2025-02-01 02:00:00</td>\n",
       "      <td>16.94</td>\n",
       "      <td>61.41</td>\n",
       "      <td>13.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1006</td>\n",
       "      <td>{'latitude': 35.668296, 'longitude': 139.765741}</td>\n",
       "      <td>active</td>\n",
       "      <td>2018</td>\n",
       "      <td>2025-01-27 14:00:00</td>\n",
       "      <td>14.38</td>\n",
       "      <td>54.03</td>\n",
       "      <td>10.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1010</td>\n",
       "      <td>{'latitude': 35.61944, 'longitude': 139.76487}</td>\n",
       "      <td>active</td>\n",
       "      <td>2020</td>\n",
       "      <td>2025-01-19 17:00:00</td>\n",
       "      <td>10.60</td>\n",
       "      <td>34.62</td>\n",
       "      <td>45.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>1010</td>\n",
       "      <td>{'latitude': 35.61944, 'longitude': 139.76487}</td>\n",
       "      <td>active</td>\n",
       "      <td>2020</td>\n",
       "      <td>2025-01-17 22:00:00</td>\n",
       "      <td>8.13</td>\n",
       "      <td>43.52</td>\n",
       "      <td>28.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>1010</td>\n",
       "      <td>{'latitude': 35.61944, 'longitude': 139.76487}</td>\n",
       "      <td>active</td>\n",
       "      <td>2020</td>\n",
       "      <td>2025-03-13 07:00:00</td>\n",
       "      <td>19.56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>1010</td>\n",
       "      <td>{'latitude': 35.61944, 'longitude': 139.76487}</td>\n",
       "      <td>active</td>\n",
       "      <td>2020</td>\n",
       "      <td>2025-03-28 00:00:00</td>\n",
       "      <td>5.79</td>\n",
       "      <td>61.62</td>\n",
       "      <td>18.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>1010</td>\n",
       "      <td>{'latitude': 35.61944, 'longitude': 139.76487}</td>\n",
       "      <td>active</td>\n",
       "      <td>2020</td>\n",
       "      <td>2025-02-21 07:00:00</td>\n",
       "      <td>9.06</td>\n",
       "      <td>52.05</td>\n",
       "      <td>59.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     station_id                                          location  status  \\\n",
       "0          1006  {'latitude': 35.668296, 'longitude': 139.765741}  active   \n",
       "1          1006  {'latitude': 35.668296, 'longitude': 139.765741}  active   \n",
       "2          1006  {'latitude': 35.668296, 'longitude': 139.765741}  active   \n",
       "3          1006  {'latitude': 35.668296, 'longitude': 139.765741}  active   \n",
       "4          1006  {'latitude': 35.668296, 'longitude': 139.765741}  active   \n",
       "..          ...                                               ...     ...   \n",
       "495        1010    {'latitude': 35.61944, 'longitude': 139.76487}  active   \n",
       "496        1010    {'latitude': 35.61944, 'longitude': 139.76487}  active   \n",
       "497        1010    {'latitude': 35.61944, 'longitude': 139.76487}  active   \n",
       "498        1010    {'latitude': 35.61944, 'longitude': 139.76487}  active   \n",
       "499        1010    {'latitude': 35.61944, 'longitude': 139.76487}  active   \n",
       "\n",
       "     deployment_year            timestamp  temperature_celsius  \\\n",
       "0               2018  2025-03-05 23:00:00                11.14   \n",
       "1               2018  2025-03-31 01:00:00                 9.97   \n",
       "2               2018  2025-01-14 09:00:00                18.95   \n",
       "3               2018  2025-02-01 02:00:00                16.94   \n",
       "4               2018  2025-01-27 14:00:00                14.38   \n",
       "..               ...                  ...                  ...   \n",
       "495             2020  2025-01-19 17:00:00                10.60   \n",
       "496             2020  2025-01-17 22:00:00                 8.13   \n",
       "497             2020  2025-03-13 07:00:00                19.56   \n",
       "498             2020  2025-03-28 00:00:00                 5.79   \n",
       "499             2020  2025-02-21 07:00:00                 9.06   \n",
       "\n",
       "     humidity_percent  pm2_5  \n",
       "0               92.50  43.29  \n",
       "1               94.61  34.22  \n",
       "2               54.45  36.74  \n",
       "3               61.41  13.78  \n",
       "4               54.03  10.38  \n",
       "..                ...    ...  \n",
       "495             34.62  45.38  \n",
       "496             43.52  28.63  \n",
       "497               NaN  56.10  \n",
       "498             61.62  18.84  \n",
       "499             52.05  59.33  \n",
       "\n",
       "[500 rows x 8 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge=pd.merge(station_info,reading,on='station_id')\n",
    "merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f453b143"
   },
   "source": [
    "## TASK 3: Clean data\n",
    "\n",
    "Filter the merged DataFrame to keep only 'active' stations and remove rows with missing values in 'temperature_celsius' or 'pm2_5'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88ecd5a3"
   },
   "source": [
    "**Reasoning**:\n",
    "Filter the merged dataframe to keep only active stations and remove rows with missing values in the specified columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "fa0aa6f0"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataFrame.dropna() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[114]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m merge=\u001b[43mmerge\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstatus\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: DataFrame.dropna() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "merge=merge.dropna(['temperature_celsius',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b6a3be6"
   },
   "source": [
    "## TASK 4: Analyze data\n",
    "\n",
    "Calculate the average 'temperature_celsius' and 'pm2_5' for each city.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "485603b4"
   },
   "source": [
    "**Reasoning**:\n",
    "Calculate the mean temperature and PM2.5 for each city by grouping the cleaned_df by city and applying the mean aggregation. Then display the head of the resulting dataframe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ac870825"
   },
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40979f5f"
   },
   "source": [
    "## TASK 5: Save report\n",
    "\n",
    "Save the city summary as a CSV file named `city_summary_report.csv` with the specified columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1842ff9e"
   },
   "source": [
    "**Reasoning**:\n",
    "Reset the index, rename the city column, and save the city summary DataFrame to a CSV file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2d720f4f"
   },
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
